{
  "name": "vllm",
  "version": "0.10.0",
  "summary": "A high-throughput and memory-efficient inference and serving engine for LLMs",
  "author": "vLLM Team",
  "license": null,
  "home_page": null,
  "download_filename": "vllm-0.10.0.tar.gz",
  "download_time": "2025-07-25T07:54:13.206025",
  "package_url": "https://pypi.org/project/vllm/"
}