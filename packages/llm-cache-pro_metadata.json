{
  "name": "llm-cache-pro",
  "version": "0.1.1",
  "summary": "A drop-in, model-agnostic cache for Large Language Model API calls",
  "author": null,
  "license": "MIT",
  "home_page": null,
  "download_filename": "llm_cache_pro-0.1.1.tar.gz",
  "download_time": "2025-07-23T17:16:18.097921",
  "package_url": "https://pypi.org/project/llm-cache-pro/"
}